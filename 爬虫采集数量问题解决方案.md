# 🔧 爬虫采集数量问题解决方案

## 🎯 问题现象

**设置**：maxCount = 10  
**实际结果**：只采集到 3 条

## 🔍 问题原因

### 根本原因

爬虫代码中的 `maxCount` 参数是**最大数量限制**，但实际采集数量取决于：

1. **网页上能解析出的链接数量** ← 主要原因
2. 过滤条件是否太严格
3. 网站页面结构是否变化

### 具体分析

查看代码可以看到：

```java
// CCTV 爬虫的链接解析
protected List<String> parseNewsLinks(Document doc, int maxCount) {
    List<String> links = new ArrayList<>();
    
    // 选择器：a[href*=cctv.com]
    Elements newsElements = doc.select("a[href*=cctv.com]");
    
    for (Element element : newsElements) {
        String href = element.attr("href");
        
        // 过滤条件很严格
        if (href.contains("/news.cctv.com/") || 
            href.contains("/m.news.cctv.com/")) {
            
            // 必须匹配日期格式：/2024/10/14/
            if (fullUrl.matches(".*\\d{4}/\\d{2}/\\d{2}/.*")) {
                links.add(fullUrl);
            }
            
            if (links.size() >= maxCount) {
                break;  // 这里会停止
            }
        }
    }
    
    return links;  // 实际可能只有3条
}
```

**问题所在：**
- 首页上符合条件的新闻链接确实只有 3 条
- 过滤条件（必须包含日期格式）太严格
- 新闻网站首页通常不会显示太多新闻链接

---

## ✅ 解决方案

### 方案1: 改进爬虫选择器（推荐）

需要修改爬虫代码，让它能从更多页面获取链接。

#### CCTV 爬虫改进

**文件：** `backend/src/main/java/com/news/crawler/impl/CctvNewsCrawler.java`

```java
@Override
protected List<String> parseNewsLinks(Document doc, int maxCount) {
    List<String> links = new ArrayList<>();
    
    try {
        // 方案1: 放宽选择器，获取更多链接
        Elements newsElements = doc.select("a[href*=news.cctv.com]");
        
        for (Element element : newsElements) {
            String href = element.attr("href");
            
            // 放宽过滤条件
            if (href.contains("news.cctv.com") && 
                !href.contains("javascript") &&
                !href.endsWith(".jpg") &&
                !href.endsWith(".png")) {
                
                String fullUrl = href.startsWith("http") ? href : buildFullUrl(BASE_URL, href);
                
                // 简化匹配条件，包含年份即可
                if (fullUrl.matches(".*\\d{4}.*") && !links.contains(fullUrl)) {
                    links.add(fullUrl);
                }
                
                if (links.size() >= maxCount) {
                    break;
                }
            }
        }
        
        // 方案2: 如果首页链接不够，尝试访问列表页
        if (links.size() < maxCount) {
            String listPageUrl = "https://news.cctv.com/2019/07/gaiban/cmsdatainterface/page/news_1.jsonp";
            // 访问更多页面获取链接
            // ...
        }
        
        log.info("Parsed {} news links from CCTV", links.size());
        
    } catch (Exception e) {
        log.error("Failed to parse news links: {}", e.getMessage());
    }
    
    return links;
}
```

#### 网易爬虫改进

**网易问题更严重**：正则表达式太严格

```java
// 原代码：
if (href.matches(".*\\d{2}/\\d{4}/.*\\.html"))  // 太严格！

// 改进后：
if (href.contains("news.163.com") && 
    href.endsWith(".html") &&
    !links.contains(href)) {
    links.add(href);
}
```

---

### 方案2: 使用API接口（更可靠）

很多新闻网站提供API接口，比使用网页爬虫更稳定。

**示例：创建 API 爬虫**

```java
@Component
@Slf4j
public class CctvApiCrawler extends AbstractNewsCrawler {
    
    @Override
    protected List<String> parseNewsLinks(Document doc, int maxCount) {
        List<String> links = new ArrayList<>();
        
        // 使用API获取新闻列表
        String apiUrl = "https://api.cctv.com/news/list?page=1&size=" + maxCount;
        
        try {
            // 调用API
            String jsonResponse = fetchJson(apiUrl);
            JSONArray newsArray = new JSONArray(jsonResponse);
            
            for (int i = 0; i < Math.min(newsArray.length(), maxCount); i++) {
                JSONObject news = newsArray.getJSONObject(i);
                String url = news.getString("url");
                links.add(url);
            }
            
        } catch (Exception e) {
            log.error("Failed to fetch from API: {}", e.getMessage());
        }
        
        return links;
    }
}
```

---

### 方案3: 使用RSS订阅（最简单）

很多新闻网站提供RSS源，这是最稳定的方式。

**RSS地址：**
- CCTV: `http://rss.cctv.com/rss/news.xml`
- 新浪: `http://news.sina.com.cn/rss/news.xml`
- 网易: `http://news.163.com/rss/news.xml`

---

### 方案4: 临时解决 - 多次采集

在当前代码不变的情况下，可以多次运行采集：

1. 第一次采集：获取3条
2. 等待几分钟（新闻更新）
3. 第二次采集：再获取3条新的
4. 重复多次直到满足需求

---

## 🛠️ 快速修复（推荐）

### 修改1: 放宽CCTV过滤条件

**文件：** `CctvNewsCrawler.java:54行`

```java
// 原代码（太严格）：
if (fullUrl.matches(".*\\d{4}/\\d{2}/\\d{2}/.*") && 
    !links.contains(fullUrl)) {
    links.add(fullUrl);
}

// 修改为（更宽松）：
if (fullUrl.contains("news.cctv.com") && 
    fullUrl.matches(".*\\d{4}.*") &&  // 只要包含年份
    !href.endsWith(".jpg") &&
    !href.endsWith(".png") &&
    !links.contains(fullUrl)) {
    links.add(fullUrl);
}
```

### 修改2: 修复网易爬虫

**文件：** `NeteaseNewsCrawler.java:49行`

```java
// 原代码（太严格）：
if (href.contains("/news.163.com/") && 
    href.matches(".*\\d{2}/\\d{4}/.*\\.html") &&
    !links.contains(href)) {

// 修改为（更宽松）：
if (href.contains("news.163.com") && 
    href.endsWith(".html") &&
    href.matches(".*\\d+.*") &&  // 包含数字即可
    !links.contains(href)) {
```

### 修改3: 新浪爬虫已经相对较好

新浪爬虫的过滤条件比较合理，但可以进一步优化。

---

## 📝 完整代码示例

以下是改进后的 CCTV 爬虫完整示例：

```java
@Override
protected List<String> parseNewsLinks(Document doc, int maxCount) {
    List<String> links = new ArrayList<>();
    
    try {
        // 1. 获取所有可能的新闻链接
        Elements newsElements = doc.select("a[href]");
        
        for (Element element : newsElements) {
            String href = element.attr("abs:href");  // 自动转换为绝对URL
            
            // 2. 基本过滤
            if (href.contains("news.cctv.com") &&
                !href.contains("javascript") &&
                !href.contains("images") &&
                !href.endsWith(".jpg") &&
                !href.endsWith(".png") &&
                !href.endsWith(".pdf") &&
                !links.contains(href)) {
                
                // 3. URL模式检查（放宽条件）
                // 接受包含年份的URL: /2024/ 或 /2025/
                if (href.matches(".*/(20\\d{2})/.*")) {
                    links.add(href);
                    log.debug("Added news link: {}", href);
                    
                    if (links.size() >= maxCount) {
                        break;
                    }
                }
            }
        }
        
        log.info("Parsed {} news links from CCTV (target: {})", links.size(), maxCount);
        
        // 4. 如果链接不够，记录警告
        if (links.size() < maxCount) {
            log.warn("Only found {} links, less than target {}", links.size(), maxCount);
        }
        
    } catch (Exception e) {
        log.error("Failed to parse news links: {}", e.getMessage(), e);
    }
    
    return links;
}
```

---

## 🧪 测试改进效果

### 测试步骤

1. **修改代码后重新编译**
```bash
cd backend
mvn clean compile
```

2. **重启后端**
```bash
mvn spring-boot:run
```

3. **测试单个爬虫**
```bash
# PowerShell
$headers = @{
    "Authorization" = "Bearer {你的token}"
}
Invoke-WebRequest -Uri "http://localhost:8080/api/crawler/trigger/cctv?maxCount=10" -Method POST -Headers $headers
```

4. **查看日志**
```bash
Get-Content backend\logs\news-management.log -Tail 50
```

应该能看到类似：
```
Parsed 10 news links from CCTV (target: 10)  # 成功！
```

---

## 📊 为什么只能爬到3条？

### 网站结构分析

我分析了一下，可能的原因：

| 新闻源 | 问题 | 原因 |
|--------|------|------|
| **CCTV** | 只有3条 | 首页只有3条符合严格日期格式的链接 |
| **网易** | 0条 | 正则表达式太严格，没有匹配到任何链接 |
| **新浪** | 3条 | 滚动页面加载更多需要JavaScript |

### 根本原因

1. **静态爬虫的局限性**
   - 只能获取HTML中的内容
   - 无法执行JavaScript
   - 很多现代网站用Ajax加载内容

2. **选择器太严格**
   - 必须完全匹配特定格式
   - 新网页结构变化导致匹配失败

3. **首页链接有限**
   - 新闻网站首页通常只展示少量精选新闻
   - 需要翻页或访问列表页才能获取更多

---

## 🚀 最佳实践建议

### 建议1: 使用动态爬虫

使用 Selenium 或 Puppeteer：
```java
// 添加依赖
<dependency>
    <groupId>org.seleniumhq.selenium</groupId>
    <artifactId>selenium-java</artifactId>
    <version>4.15.0</version>
</dependency>
```

### 建议2: 降低预期

对于简单的HTML爬虫：
- **CCTV**: 预期 3-5 条/次
- **新浪**: 预期 5-8 条/次
- **网易**: 预期 0-3 条/次（需要修复）

**采集策略**：
- 增加采集频率（每2小时一次）
- 多次采集累积数据
- 使用定时任务自动采集

### 建议3: 使用官方API

许多新闻网站提供API：
- 更稳定
- 更快速
- 数据更完整
- 不会被封禁

---

## ⚡ 立即可用的临时方案

### 方案A: 多次小批量采集

```typescript
// 修改前端代码
async function triggerMultipleCrawls() {
  for (let i = 0; i < 3; i++) {
    await triggerSingleCrawler('cctv', 5)
    await new Promise(resolve => setTimeout(resolve, 30000)) // 等待30秒
  }
  ElMessage.success('累计采集完成')
}
```

### 方案B: 使用定时任务

启用后端的定时采集功能：

**文件：** `NewsCrawlerScheduler.java`

```java
@Scheduled(cron = "0 0 */2 * * ?")  // 每2小时
public void scheduledCrawl() {
    crawlerService.executeAllCrawlers(5);  // 每次5条
}
```

---

## 📞 总结

**问题**：设置10条只采集到3条  
**原因**：网页上只能解析出3条符合条件的链接  
**解决**：
1. 修改选择器和过滤条件（代码级别）
2. 多次小批量采集（运营级别）
3. 使用API或RSS（架构级别）

**推荐做法**：
- 短期：多次采集，累积数据
- 长期：改进爬虫代码，使用API

需要我帮您修改爬虫代码吗？

